{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Biased Hiring Tool Analysis\n",
    "\n",
    "## Case Study: Amazon's AI Recruiting Tool\n",
    "\n",
    "This notebook analyzes the bias in Amazon's AI recruiting tool that penalized female candidates and proposes solutions to make it fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Job Applicant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../Data/job_applicant_dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Potential Sources of Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze gender distribution\n",
    "if 'gender' in df.columns:\n",
    "    print(\"Gender Distribution:\")\n",
    "    print(df['gender'].value_counts())\n",
    "    \n",
    "    # Visualize gender distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df['gender'].value_counts().plot(kind='bar')\n",
    "    plt.title('Gender Distribution in Dataset')\n",
    "    plt.xlabel('Gender')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Gender column not found. Looking for gender-related columns...\")\n",
    "    gender_cols = [col for col in df.columns if 'gender' in col.lower() or 'sex' in col.lower()]\n",
    "    print(f\"Potential gender columns: {gender_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze other demographic features\n",
    "demographic_cols = []\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['gender', 'sex', 'race', 'ethnicity', 'age', 'education']):\n",
    "        demographic_cols.append(col)\n",
    "\n",
    "if demographic_cols:\n",
    "    print(\"Demographic columns found:\", demographic_cols)\n",
    "    \n",
    "    # Analyze each demographic column\n",
    "    for col in demographic_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            print(f\"\\n{col} distribution:\")\n",
    "            print(df[col].value_counts())\n",
    "else:\n",
    "    print(\"No obvious demographic columns found. Let's explore the dataset structure.\")\n",
    "    print(\"\\nDataset info:\")\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Hiring Decisions by Demographic Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for target variable (hiring decision)\n",
    "target_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in ['hire', 'decision', 'employed', 'hired'])]\n",
    "\n",
    "if target_cols:\n",
    "    target_col = target_cols[0]\n",
    "    print(f\"Target column found: {target_col}\")\n",
    "    \n",
    "    # Analyze hiring rates by demographic groups\n",
    "    if demographic_cols:\n",
    "        for demo_col in demographic_cols:\n",
    "            if demo_col != target_col:\n",
    "                print(f\"\\nHiring rates by {demo_col}:\")\n",
    "                hiring_by_demo = df.groupby(demo_col)[target_col].agg(['count', 'mean'])\n",
    "                hiring_by_demo.columns = ['Total Applicants', 'Hiring Rate']\n",
    "                print(hiring_by_demo)\n",
    "else:\n",
    "    print(\"No obvious target column found. Let's explore the dataset further.\")\n",
    "    print(\"\\nUnique values in each column:\")\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() < 20:\n",
    "            print(f\"{col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we can't find obvious demographic columns, let's try to identify them\n",
    "# by looking at columns with categorical data\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# Show unique values for each categorical column\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} unique values:\")\n",
    "    print(df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulate Bias Detection (Assuming Dataset Structure)\n",
    "\n",
    "Since we need to work with the actual dataset structure, let's create a comprehensive bias analysis framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bias_by_group(df, group_col, target_col, positive_label=1):\n",
    "    \"\"\"\n",
    "    Analyze bias by comparing outcomes across different groups.\n",
    "    \"\"\"\n",
    "    if group_col not in df.columns or target_col not in df.columns:\n",
    "        print(f\"Columns {group_col} or {target_col} not found in dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Convert target to numeric if needed\n",
    "    if df[target_col].dtype == 'object':\n",
    "        # Try to convert to numeric, assuming positive_label represents positive outcome\n",
    "        df_analysis = df.copy()\n",
    "        df_analysis[target_col] = (df_analysis[target_col] == positive_label).astype(int)\n",
    "    else:\n",
    "        df_analysis = df.copy()\n",
    "    \n",
    "    # Calculate metrics by group\n",
    "    group_stats = df_analysis.groupby(group_col)[target_col].agg([\n",
    "        'count', 'sum', 'mean', 'std'\n",
    "    ]).rename(columns={\n",
    "        'count': 'Total',\n",
    "        'sum': 'Positive_Outcomes',\n",
    "        'mean': 'Positive_Rate',\n",
    "        'std': 'Std_Dev'\n",
    "    })\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    group_stats['Negative_Rate'] = 1 - group_stats['Positive_Rate']\n",
    "    \n",
    "    return group_stats\n",
    "\n",
    "def visualize_bias_analysis(group_stats, group_col, target_col):\n",
    "    \"\"\"\n",
    "    Visualize bias analysis results.\n",
    "    \"\"\"\n",
    "    if group_stats is None:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Positive rates by group\n",
    "    group_stats['Positive_Rate'].plot(kind='bar', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'Positive Rate by {group_col}')\n",
    "    axes[0, 0].set_ylabel('Positive Rate')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Total applicants by group\n",
    "    group_stats['Total'].plot(kind='bar', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'Total Applicants by {group_col}')\n",
    "    axes[0, 1].set_ylabel('Total Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Positive vs Negative outcomes\n",
    "    outcomes_df = pd.DataFrame({\n",
    "        'Positive': group_stats['Positive_Outcomes'],\n",
    "        'Negative': group_stats['Total'] - group_stats['Positive_Outcomes']\n",
    "    })\n",
    "    outcomes_df.plot(kind='bar', stacked=True, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'Outcomes Distribution by {group_col}')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Rate comparison\n",
    "    rate_comparison = pd.DataFrame({\n",
    "        'Positive_Rate': group_stats['Positive_Rate'],\n",
    "        'Negative_Rate': group_stats['Negative_Rate']\n",
    "    })\n",
    "    rate_comparison.plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Rate Comparison by {group_col}')\n",
    "    axes[1, 1].set_ylabel('Rate')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to identify the target column by looking for binary or categorical outcomes\n",
    "potential_targets = []\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() <= 10:  # Few unique values\n",
    "        potential_targets.append(col)\n",
    "\n",
    "print(\"Potential target columns (few unique values):\", potential_targets)\n",
    "\n",
    "# Show value counts for potential targets\n",
    "for col in potential_targets:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the analysis, let's assume we have identified our columns\n",
    "# For demonstration, let's work with what we have\n",
    "print(\"\\n=== BIAS ANALYSIS FRAMEWORK ===\")\n",
    "print(\"This framework can be applied once we identify the specific columns\")\n",
    "\n",
    "# Example of how the analysis would work:\n",
    "print(\"\\nExample Analysis Framework:\")\n",
    "print(\"1. Identify demographic columns (gender, race, age groups, etc.)\")\n",
    "print(\"2. Identify target column (hiring decision, employment status, etc.)\")\n",
    "print(\"3. Calculate hiring rates by demographic group\")\n",
    "print(\"4. Identify disparities between groups\")\n",
    "print(\"5. Statistical testing for significant differences\")\n",
    "print(\"6. Visualize the disparities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identify Sources of Bias in Amazon's Case\n",
    "\n",
    "Based on research about Amazon's AI recruiting tool, here are the identified sources of bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SOURCES OF BIAS IDENTIFIED ===\")\n",
    "print(\"\\n1. Training Data Bias:\")\n",
    "print(\"   - Historical hiring data predominantly from male-dominated tech industry\")\n",
    "print(\"   - Underrepresentation of female candidates in training data\")\n",
    "print(\"   - Historical biases in hiring practices were learned and amplified\")\n",
    "\n",
    "print(\"\\n2. Model Design Issues:\")\n",
    "print(\"   - Resume keyword matching favored traditionally male-associated words\")\n",
    "print(\"   - Penalized resumes containing 'women's' (as in 'women's chess club captain')\")\n",
    "print(\"   - Downgraded graduates from two all-women's colleges\")\n",
    "\n",
    "print(\"\\n3. Feature Selection Bias:\")\n",
    "print(\"   - Emphasis on keywords associated with male candidates\")\n",
    "print(\"   - Lack of gender-neutral language in the model\")\n",
    "print(\"   - Reinforcement of existing stereotypes in tech industry\")\n",
    "\n",
    "print(\"\\n4. Data Representation Issues:\")\n",
    "print(\"   - Training data from specific time periods with historical biases\")\n",
    "print(\"   - Geographic and institutional bias in data collection\")\n",
    "print(\"   - Lack of diversity in the training data sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Proposed Fixes to Make the Tool Fairer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PROPOSED FIXES FOR FAIRER HIRING TOOL ===\")\n",
    "\n",
    "print(\"\\nFix 1: Diversified and Balanced Training Data\")\n",
    "print(\"   - Collect training data from diverse industries and time periods\")\n",
    "print(\"   - Ensure balanced representation across genders, races, and backgrounds\")\n",
    "print(\"   - Include data from companies with successful diversity initiatives\")\n",
    "print(\"   - Use data augmentation techniques to address underrepresented groups\")\n",
    "\n",
    "print(\"\\nFix 2: Gender-Neutral Language Processing\")\n",
    "print(\"   - Implement gender-neutral keyword extraction\")\n",
    "print(\"   - Remove gendered language from feature importance calculations\")\n",
    "print(\"   - Use contextual analysis instead of simple keyword matching\")\n",
    "print(\"   - Develop neutral scoring for activities and experiences\")\n",
    "\n",
    "print(\"\\nFix 3: Fairness-Aware Model Architecture\")\n",
    "print(\"   - Implement fairness constraints in the model training process\")\n",
    "print(\"   - Use adversarial debiasing techniques\")\n",
    "print(\"   - Apply equal opportunity fairness metrics\")\n",
    "print(\"   - Regularize to prevent discrimination against protected groups\")\n",
    "\n",
    "print(\"\\nFix 4: Human-in-the-Loop Review Process\")\n",
    "print(\"   - Require human review of AI-rejected candidates\")\n",
    "print(\"   - Implement bias detection alerts for human reviewers\")\n",
    "print(\"   - Regular audits of AI decisions by diverse review panels\")\n",
    "print(\"   - Continuous monitoring and adjustment of the system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metrics to Evaluate Fairness Post-Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FAIRNESS EVALUATION METRICS ===\")\n",
    "\n",
    "print(\"\\n1. Demographic Parity Metrics:\")\n",
    "print(\"   - Selection Rate Parity: Hiring rates should be similar across groups\")\n",
    "print(\"   - Impact Ratio: Ratio of selection rates between protected and reference groups\")\n",
    "print(\"   - Disparate Impact: Statistical measure of adverse impact\")\n",
    "\n",
    "print(\"\\n2. Equal Opportunity Metrics:\")\n",
    "print(\"   - True Positive Rate Parity: Equal chance of qualified candidates being hired\")\n",
    "print(\"   - False Positive Rate Parity: Equal false positive rates across groups\")\n",
    "print(\"   - False Negative Rate Parity: Equal false negative rates across groups\")\n",
    "\n",
    "print(\"\\n3. Calibration Metrics:\")\n",
    "print(\"   - Well-Calibrated Probabilities: Predicted probabilities match actual outcomes\")\n",
    "print(\"   - Group Calibration: Calibration within each demographic group\")\n",
    "print(\"   - Reliability Diagrams: Visual assessment of calibration\")\n",
    "\n",
    "print(\"\\n4. Additional Fairness Metrics:\")\n",
    "print(\"   - Theil Index: Measure of inequality in outcomes\")\n",
    "print(\"   - Atkinson Index: Sensitivity to differences at different outcome levels\")\n",
    "print(\"   - Generalized Entropy Index: Overall inequality measure\")\n",
    "\n",
    "print(\"\\n5. Long-term Monitoring Metrics:\")\n",
    "print(\"   - Diversity Metrics: Track representation over time\")\n",
    "print(\"   - Retention Rates: Monitor post-hiring outcomes\")\n",
    "print(\"   - Career Progression: Track advancement opportunities\")\n",
    "print(\"   - Employee Satisfaction: Measure inclusive workplace culture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of fairness metrics\n",
    "def calculate_fairness_metrics(y_true, y_pred, sensitive_attr, positive_label=1):\n",
    "    \"\"\"\n",
    "    Calculate various fairness metrics.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    # Get unique groups\n",
    "    groups = np.unique(sensitive_attr)\n",
    "    metrics = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        # Filter data for this group\n",
    "        group_mask = (sensitive_attr == group)\n",
    "        y_true_group = y_true[group_mask]\n",
    "        y_pred_group = y_pred[group_mask]\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group).ravel()\n",
    "        \n",
    "        # Calculate rates\n",
    "        selection_rate = (tp + fp) / len(y_true_group) if len(y_true_group) > 0 else 0\n",
    "        true_positive_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        metrics[group] = {\n",
    "            'selection_rate': selection_rate,\n",
    "            'true_positive_rate': true_positive_rate,\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'count': len(y_true_group)\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_fairness_summary(metrics):\n",
    "    \"\"\"\n",
    "    Print a summary of fairness metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== FAIRNESS METRICS SUMMARY ===\")\n",
    "    \n",
    "    for group, values in metrics.items():\n",
    "        print(f\"\\nGroup {group}:\")\n",
    "        print(f\"  Count: {values['count']}\")\n",
    "        print(f\"  Selection Rate: {values['selection_rate']:.3f}\")\n",
    "        print(f\"  True Positive Rate: {values['true_positive_rate']:.3f}\")\n",
    "        print(f\"  False Positive Rate: {values['false_positive_rate']:.3f}\")\n",
    "    \n",
    "    # Calculate disparities\n",
    "    groups = list(metrics.keys())\n",
    "    if len(groups) >= 2:\n",
    "        ref_group = groups[0]\n",
    "        print(f\"\\n=== DISPARITY ANALYSIS (vs {ref_group}) ===\")\n",
    "        \n",
    "        for group in groups[1:]:\n",
    "            print(f\"\\nGroup {group} vs {ref_group}:\")\n",
    "            print(f\"  Selection Rate Ratio: {metrics[group]['selection_rate']/metrics[ref_group]['selection_rate']:.3f}\")\n",
    "            print(f\"  TPR Ratio: {metrics[group]['true_positive_rate']/metrics[ref_group]['true_positive_rate']:.3f}\")\n",
    "            print(f\"  FPR Ratio: {metrics[group]['false_positive_rate']/metrics[ref_group]['false_positive_rate']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Recommendations\n",
    "\n",
    "This analysis framework provides a comprehensive approach to identifying and addressing bias in hiring tools. The key takeaways are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONCLUSION AND RECOMMENDATIONS ===\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Bias in AI hiring tools often stems from historical data and design choices\")\n",
    "print(\"2. Multiple sources of bias can compound and create significant disparities\")\n",
    "print(\"3. Fairness requires continuous monitoring and multiple evaluation metrics\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Implement comprehensive bias detection before deployment\")\n",
    "print(\"2. Use diverse training data and fairness-aware algorithms\")\n",
    "print(\"3. Maintain human oversight and regular audits\")\n",
    "print(\"4. Monitor long-term outcomes beyond just hiring rates\")\n",
    "print(\"5. Establish clear accountability for AI decisions\")\n",
    "\n",
    "print(\"\\nFinal Note:\")\n",
    "print(\"Creating fair AI systems requires ongoing effort and commitment to\")\n",
    "print(\"diversity, equity, and inclusion principles. Technology alone cannot\")\n",
    "print(\"solve systemic issues, but it can be designed to avoid amplifying\")\n",
    "print(\"existing biases and to support fairer decision-making processes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
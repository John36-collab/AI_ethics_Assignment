{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: COMPAS Dataset Bias Audit\n",
    "\n",
    "## Task: Audit a Dataset for Bias\n",
    "\n",
    "This notebook uses Python and AI Fairness 360 (IBM's toolkit) to analyze racial bias in COMPAS risk scores.\n",
    "\n",
    "### Dataset: COMPAS Recidivism Dataset\n",
    "\n",
    "The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm is used to assess the likelihood of a defendant reoffending. This analysis examines potential racial biases in the risk assessment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip if not already installed.\"\"\"\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        print(f\"{package_name} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "\n",
    "# Install required packages\n",
    "packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'aif360', 'tensorflow']\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AI Fairness 360 imports\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing AdversarialDebiasing\n",
    "from aif360.algorithms.postprocessing import CalibratedEqualizedOdds\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the COMPAS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the COMPAS dataset\n",
    "df = pd.read_csv('../Data/compas-scores-two-years-violent.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Basic data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for analysis\n",
    "columns_to_keep = [\n",
    "    'age', 'c_charge_degree', 'race', 'age_cat', 'score_text',\n",
    "    'sex', 'priors_count', 'days_b_screening_arrest', 'c_jail_in',\n",
    "    'c_jail_out', 'decile_score', 'is_recid', 'is_violent_recid',\n",
    "    'two_year_recid'\n",
    "]\n",
    "\n",
    "# Filter columns that exist in the dataset\n",
    "available_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "df_filtered = df[available_columns].copy()\n",
    "\n",
    "print(f\"Selected {len(available_columns)} columns from {len(columns_to_keep)} requested\")\n",
    "print(\"Selected columns:\", available_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_filtered.isnull().sum())\n",
    "\n",
    "# Drop rows with critical missing values\n",
    "df_clean = df_filtered.dropna(subset=['race', 'age', 'priors_count', 'decile_score', 'two_year_recid'])\n",
    "\n",
    "print(f\"\\nDataset shape after cleaning: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df_filtered) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze race distribution\n",
    "print(\"Race Distribution:\")\n",
    "print(df_clean['race'].value_counts())\n",
    "\n",
    "# Visualize race distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_clean['race'].value_counts().plot(kind='bar')\n",
    "plt.title('Race Distribution in COMPAS Dataset')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recidivism by race\n",
    "print(\"Recidivism rates by race:\")\n",
    "recidivism_by_race = df_clean.groupby('race')['two_year_recid'].agg(['count', 'mean'])\n",
    "recidivism_by_race.columns = ['Total', 'Recidivism_Rate']\n",
    "print(recidivism_by_race)\n",
    "\n",
    "# Visualize recidivism rates by race\n",
    "plt.figure(figsize=(12, 6))\n",
    "recidivism_by_race['Recidivism_Rate'].plot(kind='bar')\n",
    "plt.title('Recidivism Rates by Race')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Recidivism Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze COMPAS risk scores by race\n",
    "print(\"COMPAS Risk Score Distribution by Race:\")\n",
    "risk_by_race = df_clean.groupby('race')['decile_score'].agg(['mean', 'std', 'count'])\n",
    "print(risk_by_race)\n",
    "\n",
    "# Visualize risk score distributions\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=df_clean, x='race', y='decile_score')\n",
    "plt.title('COMPAS Risk Score Distribution by Race')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Risk Score (1-10)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create BinaryLabelDataset for AI Fairness 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for AI Fairness 360\n",
    "# We'll use race as the sensitive attribute\n",
    "# and two_year_recid as the label\n",
    "\n",
    "# Create a binary version of the risk score (high risk = score >= 5)\n",
    "df_clean['high_risk'] = (df_clean['decile_score'] >= 5).astype(int)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_sex = LabelEncoder()\n",
    "df_clean['sex_encoded'] = le_sex.fit_transform(df_clean['sex'])\n",
    "\n",
    "# Create race mapping for binary privileged/unprivileged groups\n",
    "# We'll use Caucasian as the privileged group for this analysis\n",
    "race_mapping = {race: 0 for race in df_clean['race'].unique()}\n",
    "race_mapping['Caucasian'] = 1  # Privileged group\n",
    "df_clean['race_binary'] = df_clean['race'].map(race_mapping)\n",
    "\n",
    "print(\"Race mapping (1 = Caucasian/privileged, 0 = other):\")\n",
    "for race, code in race_mapping.items():\n",
    "    print(f\"  {race}: {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for the model\n",
    "feature_columns = ['age', 'priors_count', 'sex_encoded']\n",
    "sensitive_attr = 'race_binary'\n",
    "label_column = 'high_risk'\n",
    "\n",
    "# Create the dataset\n",
    "X = df_clean[feature_columns].values\n",
    "y = df_clean[label_column].values\n",
    "sensitive_attributes = df_clean[sensitive_attr].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X, y, sensitive_attributes, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Feature columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
    "metadata": {},
   "outputs": [],
   "source": [
    "# Create BinaryLabelDataset objects\n",
    "train_dataset = BinaryLabelDataset(\n",
    "    df=pd.DataFrame(np.column_stack([X_train, y_train, sens_train]),\n",
    "                   columns=feature_columns + [label_column, sensitive_attr]),\n",
    "    label_names=[label_column],\n",
    "    protected_attribute_names=[sensitive_attr],\n",
    "    favorable_label=0,  # Low risk is favorable\n",
    "    unfavorable_label=1  # High risk is unfavorable\n",
    ")\n",
    "\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    df=pd.DataFrame(np.column_stack([X_test, y_test, sens_test]),\n",
    "                   columns=feature_columns + [label_column, sensitive_attr]),\n",
    "    label_names=[label_column],\n",
    "    protected_attribute_names=[sensitive_attr],\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1\n",
    ")\n",
    "\n",
    "print(\"BinaryLabelDataset created successfully\")\n",
    "print(f\"Training dataset shape: {train_df.shape}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Initial Model and Measure Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a baseline model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fairness metrics\n",
    "def calculate_fairness_metrics(dataset, predictions, privileged_groups, unprivileged_groups):\n",
    "    \"\"\"\n",
    "    Calculate various fairness metrics using AI Fairness 360.\n",
    "    \"\"\"\n",
    "    # Create a dataset with predictions\n",
    "    pred_dataset = dataset.copy()\n",
    "    pred_dataset.labels = predictions.reshape(-1, 1)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metric_pred = ClassificationMetric(dataset, pred_dataset,\n",
    "                                     unprivileged_groups=unprivileged_groups,\n",
    "                                     privileged_groups=privileged_groups)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Statistical Parity Difference': metric_pred.statistical_parity_difference(),\n",
    "        'Equal Opportunity Difference': metric_pred.equal_opportunity_difference(),\n",
    "        'Average Odds Difference': metric_pred.average_odds_difference(),\n",
    "        'Theil Index': metric_pred.theil_index(),\n",
    "        'Disparate Impact': metric_pred.disparate_impact()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Define privileged and unprivileged groups\n",
    "privileged_groups = [{sensitive_attr: 1}]  # Caucasian\n",
    "unprivileged_groups = [{sensitive_attr: 0}]  # Other races\n",
    "\n",
    "# Calculate fairness metrics\n",
    "fairness_metrics = calculate_fairness_metrics(test_dataset, y_pred, privileged_groups, unprivileged_groups)\n",
    "\n",
    "print(\"\\n=== FAIRNESS METRICS ===\")\n",
    "for metric, value in fairness_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Bias Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive bias visualization\n",
    "def plot_bias_analysis(df_clean, sensitive_attr='race', label_col='high_risk'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for bias analysis.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Risk score distribution by race\n",
    "    sns.boxplot(data=df_clean, x=sensitive_attr, y='decile_score', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Risk Score Distribution by Race')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. High risk rates by race\n",
    "    high_risk_by_race = df_clean.groupby(sensitive_attr)[label_col].mean()\n",
    "    high_risk_by_race.plot(kind='bar', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('High Risk Rates by Race')\n",
    "    axes[0, 1].set_ylabel('High Risk Rate')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Actual recidivism by race\n",
    "    actual_recidivism = df_clean.groupby(sensitive_attr)['two_year_recid'].mean()\n",
    "    actual_recidivism.plot(kind='bar', ax=axes[0, 2])\n",
    "    axes[0, 2].set_title('Actual Recidivism by Race')\n",
    "    axes[0, 2].set_ylabel('Recidivism Rate')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Confusion matrix by race\n",
    "    races = df_clean[sensitive_attr].unique()\n",
    "    if len(races) <= 4:  # Only plot if manageable number of races\n",
    "        for i, race in enumerate(races):\n",
    "            race_data = df_clean[df_clean[sensitive_attr] == race]\n",
    "            cm = confusion_matrix(race_data['two_year_recid'], (race_data['decile_score'] >= 5))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, i])\n",
    "            axes[1, i].set_title(f'Confusion Matrix - {race}')\n",
    "            axes[1, i].set_xlabel('Predicted High Risk')\n",
    "            axes[1, i].set_ylabel('Actual Recidivism')\n",
    "    else:\n",
    "        # Show overall confusion matrix if too many races\n",
    "        cm = confusion_matrix(df_clean['two_year_recid'], (df_clean['decile_score'] >= 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Overall Confusion Matrix')\n",
    "        axes[1, 0].set_xlabel('Predicted High Risk')\n",
    "        axes[1, 0].set_ylabel('Actual Recidivism')\n",
    "        \n",
    "    # 5. False positive rates by race\n",
    "    fpr_by_race = []\n",
    "    for race in races:\n",
    "        race_data = df_clean[df_clean[sensitive_attr] == race]\n",
    "        tn, fp, fn, tp = confusion_matrix(race_data['two_year_recid'], (race_data['decile_score'] >= 5)).ravel()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fpr_by_race.append(fpr)\n",
    "    \n",
    "    fpr_df = pd.DataFrame({'Race': races, 'False Positive Rate': fpr_by_race})\n",
    "    fpr_df.plot(kind='bar', x='Race', y='False Positive Rate', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('False Positive Rates by Race')\n",
    "    axes[1, 1].set_ylabel('False Positive Rate')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 6. Fairness metrics visualization\n",
    "    metrics_names = list(fairness_metrics.keys())\n",
    "    metrics_values = list(fairness_metrics.values())\n",
    "    \n",
    "    # Create bar plot for metrics\n",
    "    bars = axes[1, 2].bar(range(len(metrics_names)), metrics_values)\n",
    "    axes[1, 2].set_title('Fairness Metrics')\n",
    "    axes[1, 2].set_xticks(range(len(metrics_names)))\n",
    "    axes[1, 2].set_xticklabels(metrics_names, rotation=45, ha='right')\n",
    "    axes[1, 2].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Color bars based on fairness\n",
    "    for i, (bar, value) in enumerate(zip(bars, metrics_values)):\n",
    "        if abs(value) > 0.1:  # Significant bias\n",
    "            bar.set_color('red')\n",
    "        elif abs(value) > 0.05:  # Moderate bias\n",
    "            bar.set_color('orange')\n",
    "        else:  # Fair\n",
    "            bar.set_color('green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate bias analysis visualizations\n",
    "plot_bias_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: ROC curves by race\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate ROC curves for each race\n",
    "races = df_clean['race'].unique()\n",
    "for race in races:\n",
    "    race_data = df_clean[df_clean['race'] == race]\n",
    "    \n",
    "    # Use decile_score as prediction (higher score = higher risk)\n",
    "    y_true = race_data['two_year_recid']\n",
    "    y_scores = race_data['decile_score'] / 10.0  # Normalize to 0-1\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{race} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves by Race')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply Bias Mitigation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Reweighing preprocessing technique\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "RW_train_dataset = RW.fit_transform(train_dataset)\n",
    "\n",
    "print(\"Reweighing applied to training dataset\")\n",
    "print(f\"Original dataset size: {len(train_dataset.df)}\")\n",
    "print(f\"Reweighted dataset size: {len(RW_train_dataset.df)}\")\n",
    "\n",
    "# Train model with reweighted data\n",
    "model_rw = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_rw.fit(RW_train_dataset.features, RW_train_dataset.labels.ravel(),\n",
    "           sample_weight=RW_train_instance.instance_weights)\n",
    "\n",
    "# Make predictions with reweighted model\n",
    "y_pred_rw = model_rw.predict(X_test)\n",
    "\n",
    "# Calculate fairness metrics for reweighted model\n",
    "fairness_metrics_rw = calculate_fairness_metrics(test_dataset, y_pred_rw, privileged_groups, unprivileged_groups)\n",
    "\n",
    "print(\"\\n=== FAIRNESS METRICS AFTER REWEIGHING ===\")\n",
    "for metric, value in fairness_metrics_rw.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare fairness metrics before and after mitigation\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': list(fairness_metrics.keys()),\n",
    "    'Before Mitigation': list(fairness_metrics.values()),\n",
    "    'After Reweighing': list(fairness_metrics_rw.values())\n",
    "})\n",
    "\n",
    "print(\"\\nFairness Metrics Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, comparison_df['Before Mitigation'], width, label='Before', alpha=0.8)\n",
    "plt.bar(x + width/2, comparison_df['After Reweighing'], width, label='After Reweighing', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Fairness Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Fairness Metrics Before and After Mitigation')\n",
    "plt.xticks(x, comparison_df['Metric'], rotation=45, ha='right')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Bias Audit Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audit_report(fairness_metrics, fairness_metrics_mitigated, dataset_info):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive bias audit report.\n",
    "    \"\"\"\n",
    "    report = \"\"\"\n",
    "    # COMPAS Dataset Bias Audit Report\n",
    "    \n",
    "    ## Executive Summary\n",
    "    This report presents the findings of a bias audit conducted on the COMPAS\n",
    "    (Correctional Offender Management Profiling for Alternative Sanctions)\n",
    "    recidivism risk assessment tool. The analysis focuses on identifying racial\n",
    "    biases in the risk score predictions and evaluating the effectiveness of\n",
    "    bias mitigation techniques.\n",
    "    \n",
    "    ## Dataset Overview\n",
    "    \"\"\"\n",
    "    \n",
    "    report += f\"- Total records analyzed: {dataset_info['total_records']:,}\\n\"\n",
    "    report += f\"- Racial groups represented: {len(dataset_info['race_distribution'])}\\n\"\n",
    "    report += f\"- Time period: Two-year recidivism follow-up\\n\"\n",
    "    report += f\"- Protected attribute: Race (Caucasian vs. non-Caucasian)\\n\\n\"\n",
    "    \n",
    "    report += \"## Key Findings\\n\\n\"\n",
    "    \n",
    "    # Analyze statistical parity\n",
    "    spd = fairness_metrics['Statistical Parity Difference']\n",
    "    spd_mitigated = fairness_metrics_mitigated['Statistical Parity Difference']\n",
    "    \n",
    "    report += f\"### 1. Statistical Parity Difference\\n\"\n",
    "    report += f\"- Before mitigation: {spd:.4f}\\n\"\n",
    "    report += f\"- After mitigation: {spd_mitigated:.4f}\\n\"\n",
    "    \n",
    "    if abs(spd) > 0.1:\n",
    "        report += f\"- **Significant disparity detected** (|value| > 0.1)\\n\"\n",
    "    else:\n",
    "        report += f\"- Acceptable parity level\\n\"\n",
    "    \n",
    "    report += f\"- Interpretation: {'Higher risk scores assigned to non-Caucasian defendants' if spd < 0 else 'Higher risk scores assigned to Caucasian defendants'}\\n\\n\"\n",
    "    \n",
    "    # Analyze equal opportunity\n",
    "    eod = fairness_metrics['Equal Opportunity Difference']\n",
    "    eod_mitigated = fairness_metrics_mitigated['Equal Opportunity Difference']\n",
    "    \n",
    "    report += f\"### 2. Equal Opportunity Difference\\n\"\n",
    "    report += f\"- Before mitigation: {eod:.4f}\\n\"\n",
    "    report += f\"- After mitigation: {eod_mitigated:.4f}\\n\"\n",
    "    \n",
    "    if abs(eod) > 0.1:\n",
    "        report += f\"- **Significant disparity in true positive rates detected**\\n\"\n",
    "    else:\n",
    "        report += f\"- Acceptable equal opportunity level\\n\"\n",
    "    \n",
    "    report += f\"- Interpretation: {'Non-Caucasian defendants have lower true positive rates' if eod < 0 else 'Caucasian defendants have lower true positive rates'}\\n\\n\"\n",
    "    \n",
    "    # Analyze disparate impact\n",
    "    di = fairness_metrics['Disparate Impact']\n",
    "    di_mitigated = fairness_metrics_mitigated['Disparate Impact']\n",
    "    \n",
    "    report += f\"### 3. Disparate Impact\\n\"\n",
    "    report += f\"- Before mitigation: {di:.4f}\\n\"\n",
    "    report += f\"- After mitigation: {di_mitigated:.4f}\\n\"\n",
    "    \n",
    "    if di < 0.8:\n",
    "        report += f\"- **Significant adverse impact detected** (value < 0.8)\\n\"\n",
    "    elif di > 1.25:\n",
    "        report += f\"- **Significant favorable impact detected** (value > 1.25)\\n\"\n",
    "    else:\n",
    "        report += f\"- Acceptable disparate impact level\\n\"\n",
    "    \n",
    "    report += f\"- Interpretation: {'Non-Caucasian defendants are disadvantaged' if di < 1 else 'Caucasian defendants are disadvantaged'}\\n\\n\"\n",
    "    \n",
    "    report += \"## Bias Mitigation Effectiveness\\n\\n\"\n",
    "    \n",
    "    # Overall assessment\n",
    "    improvement_count = 0\n",
    "    for metric in ['Statistical Parity Difference', 'Equal Opportunity Difference', 'Disparate Impact']:\n",
    "        before = abs(fairness_metrics[metric])\n",
    "        after = abs(fairness_metrics_mitigated[metric])\n",
    "        if after < before:\n",
    "            improvement_count += 1\n",
    "    \n",
    "    report += f\"- {improvement_count}/3 key metrics showed improvement after mitigation\\n\"\n",
    "    \n",
    "    if improvement_count >= 2:\n",
    "        report += \"- **Overall positive effect observed from bias mitigation techniques**\\n\"\n",
    "    else:\n",
    "        report += \"- Limited improvement observed; additional mitigation strategies recommended\\n\"\n",
    "    \n",
    "    report += \"\\n## Recommendations\\n\\n\"\n",
    "    report += \"### Immediate Actions:\\n\"\n",
    "    report += \"1. Implement regular bias monitoring in the COMPAS system\\n\"\n",
    "    report += \"2. Use multiple fairness metrics for comprehensive evaluation\\n\"\n",
    "    report += \"3. Consider additional bias mitigation techniques\\n\\n\"\n",
    "    \n",
    "    report += \"### Long-term Strategies:\\n\"\n",
    "    report += \"1. Diversify training data to include more representative samples\\n\"\n",
    "    report += \"2. Implement human oversight for high-stakes decisions\\n\"\n",
    "    report += \"3. Conduct regular audits and bias assessments\\n\"\n",
    "    report += \"4. Develop transparent documentation of model limitations\\n\\n\"\n",
    "    \n",
    "    report += \"## Conclusion\\n\"\n",
    "    report += \"The COMPAS risk assessment tool shows measurable racial disparities in its predictions.\"\n",
    "    report += \" While bias mitigation techniques show some promise, ongoing monitoring and improvement \"\n",
    "    report += \"are essential to ensure fair and equitable treatment of all defendants.\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the audit report\n",
    "dataset_info = {\n",
    "    'total_records': len(df_clean),\n",
    "    'race_distribution': df_clean['race'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "audit_report = generate_audit_report(fairness_metrics, fairness_metrics_rw, dataset_info)\n",
    "\n",
    "print(audit_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the audit report to a file\n",
    "with open('compas_bias_audit_report.txt', 'w') as f:\n",
    "    f.write(audit_report)\n",
    "\n",
    "print(\"\\nAudit report saved to 'compas_bias_audit_report.txt'\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Dataset size: {len(df_clean):,} records\")\n",
    "print(f\"Racial groups: {df_clean['race'].nunique()}\")\n",
    "print(f\"Overall recidivism rate: {df_clean['two_year_recid'].mean():.3f}\")\n",
    "print(f\"Overall high risk rate: {df_clean['high_risk'].mean():.3f}\")\n",
    "print(f\"Model accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(\"This analysis provides a comprehensive audit of racial bias in the COMPAS dataset\")\n",
    "print(\"using AI Fairness 360 metrics and visualization techniques.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}